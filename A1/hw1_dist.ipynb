{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Exploring Zipf's Law\n",
    "\n",
    "#### Due date: January 13, 11:59 PM\n",
    "\n",
    "For this assignment, you will submit two files on Gradescope, `hw1_dist.ipynb` and `hw1_dist.pdf`. `hw1_dist.ipynb` goes under \"Assignment 1 - Code\" and `hw1_dist.pdf` goes under \"Assignment 1 - Written.\"\n",
    "\n",
    "### What is Zipf's Law?\n",
    "\n",
    "Zipf's Law is an empirical observation that in any large corpus of natural language, the frequency of a word is inversely proportional to its frequency rank. Zipf's Law is a special case of a power law distribution.\n",
    "\n",
    "More generally, the relationship between rank ($r$) and frequency ($f$) can be modeled as:\n",
    "$$f \\propto \\frac{1}{r^s}$$\n",
    "where $s$ is the Zipf exponent (for natural language, $s \\approx 1$).\n",
    "\n",
    "#### Log-Space Derivation\n",
    "To see why this appears as a straight line on a log-log plot, we take the logarithm of both sides:\n",
    "1. Start with the power law: $f = \\frac{C}{r^s}$\n",
    "2. Take the log: $\\log(f) = \\log\\left(\\frac{C}{r^s}\\right)$\n",
    "3. Use log properties: $\\log(f) = \\log(C) - \\log(r^s)$\n",
    "4. Final linear form: $\\log(f) = \\log(C) - s \\cdot \\log(r)$\n",
    "\n",
    "Comparing this to the standard linear equation $y = mx + b$, where $y = \\log(f)$ and $x = \\log(r)$:\n",
    "- The slope ($m$) is $-s$.\n",
    "- The y-intercept ($b$) is $\\log(C)$.\n",
    "\n",
    "For natural language where $s \\approx 1$, we expect a slope of approximately -1.0. This linear signature is a unique property of natural language and is notably absent in randomly generated text.\n",
    "\n",
    "#### Relationship Between Zipf Exponent $s$ and Distribution Exponent $\\alpha$\n",
    "\n",
    "The Zipf exponent $s$ describes the rank-frequency relationship ($f \\propto r^{-s}$). There is a related but distinct exponent $\\alpha$ that describes the probability distribution of word frequencies: $P(f) \\propto f^{-\\alpha}$.\n",
    "\n",
    "The relationship between them is: $\\alpha = 1 + \\frac{1}{s}$, or equivalently $s = \\frac{1}{\\alpha - 1}$.\n",
    "\n",
    "For natural language where $s \\approx 1$, we expect $\\alpha \\approx 2$.\n",
    "\n",
    "#### Testing Power Law Fit\n",
    "To rigorously test whether a distribution follows a power law, we use the method from Clauset, Shalizi, and Newman (2009), \"Power-law distributions in empirical data\" (https://arxiv.org/abs/0706.1062). This approach:\n",
    "\n",
    "1. **Estimates $\\alpha$**: Fits the power law exponent using maximum likelihood estimation.\n",
    "\n",
    "2. **Compares alternatives**: Tests whether power law is a better fit than exponential or lognormal distributions using likelihood ratio tests.\n",
    "\n",
    "3. **The $x_{min}$ parameter**: An important parameter is $x_{min}$, the minimum frequency value above which the power law is fitted. Words with very low frequencies often deviate from the power law pattern. In this exercise, we set $x_{min} = 5$. You can read the reference above for more details on how to choose $x_{min}$, and you can try other values in the assignment.\n",
    "\n",
    "### Assignment Overview\n",
    "In this assignment, we will provide an implementation for a basic version of the following workflows.\n",
    "1. Tokenization: A function to return a list of words given a document.\n",
    "2. Frequency Counting: Compute word occurrences across your corpus.\n",
    "3. Visualization: Plot the frequency-rank relationship on a log-log scale.\n",
    "4. Parameter Estimation: Estimate the distribution exponent $\\alpha$, derive the Zipf exponent $s$, and test whether power law is supported.\n",
    "\n",
    "Your task will be:\n",
    "1. Run the pipeline on a human corpus and interpret the results.\n",
    "2. Run the pipeline on an AI corpus and interpret the results.\n",
    "3. Run the pipeline on a mystery corpus and explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the functions in the below cell in your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def count_word_frequencies(text, tokenizer):\n",
    "    tokens = tokenizer(text)\n",
    "    frequencies = Counter(tokens)\n",
    "    return frequencies\n",
    "\n",
    "def fit_zipf(frequencies, xmin=5, plot=True):\n",
    "    \"\"\"\n",
    "    Fit a power law to word frequencies and test goodness of fit.\n",
    "    \n",
    "    Uses the method from Clauset, Shalizi, and Newman (2009):\n",
    "    \"Power-law distributions in empirical data\"\n",
    "    https://arxiv.org/abs/0706.1062\n",
    "    \n",
    "    Args:\n",
    "        frequencies: Dict or Counter mapping tokens to counts.\n",
    "        xmin: Minimum frequency value for power law fitting (default=5).\n",
    "        plot: Whether to plot the PDF with fitted power law.\n",
    "    Returns:\n",
    "        Dict with fit results including alpha, s, and comparison verdicts.\n",
    "    \"\"\"\n",
    "    freq_values = np.array(list(frequencies.values()))\n",
    "    \n",
    "    # Fit power law distribution using Clauset et al. method\n",
    "    fit = powerlaw.Fit(freq_values, xmin=xmin, discrete=True, verbose=False)\n",
    "    \n",
    "    # Derive Zipf exponent s from alpha: alpha = 1 + 1/s, so s = 1/(alpha - 1)\n",
    "    alpha = fit.power_law.alpha\n",
    "    s = 1 / (alpha - 1)\n",
    "    \n",
    "    # Compare power law to exponential distribution\n",
    "    R_exp, p_exp = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)\n",
    "    \n",
    "    # Compare power law to lognormal distribution\n",
    "    R_log, p_log = fit.distribution_compare('power_law', 'lognormal', normalized_ratio=True)\n",
    "    \n",
    "    # Determine verdict for each comparison\n",
    "    def get_verdict(R, p, alt_name):\n",
    "        if p >= 0.05:\n",
    "            return f\"indistinguishable from {alt_name}\"\n",
    "        elif R > 0:\n",
    "            return f\"power law preferred over {alt_name}\"\n",
    "        else:\n",
    "            return f\"{alt_name} preferred over power law\"\n",
    "    \n",
    "    verdict_exp = get_verdict(R_exp, p_exp, \"exponential\")\n",
    "    verdict_log = get_verdict(R_log, p_log, \"lognormal\")\n",
    "    \n",
    "    result = {\n",
    "        'alpha': alpha,\n",
    "        's': s,\n",
    "        'xmin': xmin,\n",
    "        'R_vs_exponential': R_exp,\n",
    "        'p_vs_exponential': p_exp,\n",
    "        'R_vs_lognormal': R_log,\n",
    "        'p_vs_lognormal': p_log,\n",
    "        'verdict_exponential': verdict_exp,\n",
    "        'verdict_lognormal': verdict_log,\n",
    "    }\n",
    "    \n",
    "    print(f\"alpha: {alpha:.3f} (xmin={xmin})\")\n",
    "    print(f\"Zipf exponent s: {s:.3f} (derived from alpha)\")\n",
    "    print(f\"vs Exponential: R={R_exp:.3f}, p={p_exp:.3f} -> {verdict_exp}\")\n",
    "    print(f\"vs Lognormal:   R={R_log:.3f}, p={p_log:.3f} -> {verdict_log}\")\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        fit.plot_pdf(ax=ax, color='b', label='Data')\n",
    "        fit.power_law.plot_pdf(ax=ax, color='r', linestyle='--', label='Power law fit')\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.set_ylabel('PDF')\n",
    "        ax.set_title(\"Zipf's Law\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def zipf_law_main(documents, xmin=5, tokenizer=tokenize, plot=True):\n",
    "    frequencies = count_word_frequencies(documents, tokenizer)\n",
    "    result = fit_zipf(frequencies, xmin=xmin, plot=plot)\n",
    "    return result, frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with Moby Dick\n",
    "\n",
    "For this exercise, we will analyze **Moby Dick** from the NLTK Gutenberg corpus. Run the given code and answer the following questions.\n",
    "\n",
    "(a) Plot the Zipf's law graph for the corpus. What is the estimated $s$? Is power law supported? (10 points)\n",
    "\n",
    "(b) How many words are there in the corpus? What is the vocabulary size? What are the 20 most common and least common words? (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Moby Dick from moby_dick.txt...\")\n",
    "with open(\"./data/moby_dick.txt\", \"r\") as fw:\n",
    "    moby_dick = fw.read()[:950000]\n",
    "\n",
    "print(f\"Loaded Moby Dick.\", moby_dick[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1a. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1b. YOUR CODE HERE\n",
    "# Put your results in the variables below\n",
    "\n",
    "total_words_moby_dick = #\n",
    "vocab_size_moby_dick = #\n",
    "\n",
    "top_20_moby_dick = #\n",
    "least_20_moby_dick = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with an AI-generated corpus\n",
    "\n",
    "Next, we will analyze an AI-generated corpus and compare it against *Moby Dick* (`data/ai_novel.txt`). The corpus contains all of the books in `GOAT-AI/generated-novels` on HuggingFace. It was minimally processed to split words concatenated together due to PDF extraction errors.\n",
    "\n",
    "(a) Plot the Zipf's Law graph and report the fitted exponent $s$ for the AI corpus. How does it compare to *Moby Dick*? (10 points)\n",
    "\n",
    "(b) How many words are there in the corpus? What is the vocabulary size? What are the 20 most common and least common words? (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading AI novel from moby_dick.txt...\")\n",
    "with open(\"./data/ai_novel.txt\", \"r\") as handle:\n",
    "    ai_novel = handle.read()\n",
    "\n",
    "print(f\"Loaded AI novel.\", ai_novel[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2a. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2b. YOUR CODE HERE\n",
    "# Put your results in the variables below\n",
    "\n",
    "total_words_ai_novel = #\n",
    "vocab_size_ai_novel = #\n",
    "\n",
    "# Get the 20 most common and least common words\n",
    "top_20_ai_novel = #\n",
    "least_20_ai_novel = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the differences between the two corpora and explain why the observations are different.\n",
    "\n",
    "(a) Compute the following quantities for the two corpora:\n",
    "- `total_words`: all words in the corpus\n",
    "- `vocab_size`: the number of unique words\n",
    "- `type_token_ratio`: `vocab_size / total_words`\n",
    "- `hapax_ratio`: hapax legomena, fraction of words that appear exactly once\n",
    "- `top10_coverage`: fraction of tokens covered by the top 10 words\n",
    "- `top100_coverage`: fraction of tokens covered by the top 100 words\n",
    "- `top1000_coverage`: fraction of tokens covered by the top 1000 words\n",
    "- `top2000_coverage`: fraction of tokens covered by the top 2000 words\n",
    "\n",
    "Optional: you are encouraged to perform more analyses for comparison. For example, you can plot the two Zipf's Law lines on the same plot and look at the visual differences. (10 points)\n",
    "\n",
    "(b) Based on your calculations in part (a), produce a short write-up comparing Moby Dick and the AI corpus. Put your final answer in the `Answer` cell below. (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3a. YOUR CODE HERE\n",
    "\n",
    "moby_dick_stats = {\n",
    "    'total_words': ,\n",
    "    'vocab_size': ,\n",
    "    'type_token_ratio': ,\n",
    "    'hapax_ratio': ,\n",
    "    'top10_coverage': ,\n",
    "    'top100_coverage': ,\n",
    "    'top1000_coverage': ,\n",
    "    'top2000_coverage': ,\n",
    "}\n",
    "\n",
    "ai_novel_stats = {\n",
    "    'total_words': ,\n",
    "    'vocab_size': ,\n",
    "    'type_token_ratio': ,\n",
    "    'hapax_ratio': ,\n",
    "    'top10_coverage': ,\n",
    "    'top100_coverage': ,\n",
    "    'top1000_coverage': ,\n",
    "    'top2000_coverage': ,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyzing a mystery corpus\n",
    "\n",
    "You are given a mystery corpus that does not follow Zipf's Law. Investigate this corpus and give a reason for why it deviates from Zipf's Law. Put your answer in the `Answer` cell below. (20 points)\n",
    "\n",
    "Hint: you should explore ways to look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading mystery corpus from mystery.txt...\")\n",
    "with open(\"./data/mystery.txt\", \"r\") as fin:\n",
    "    mystery = fin.read()\n",
    "\n",
    "print(f\"Loaded mystery corpus: \", mystery[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
