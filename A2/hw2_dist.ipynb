{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d9f96e",
   "metadata": {},
   "source": [
    "# Assignment 2: Text classification and Word2Vec\n",
    "\n",
    "**Due date**: January 23, 11:59 PM\n",
    "\n",
    "For this assignment, you will need to submit two files on Gradescope, `hw2_dist.ipynb` and `hw2_dist.pdf`.  `hw2_dist.ipynb` goes under \"Assignment 2 - Code\" and `hw2_dist.pdf` goes under \"Assignment 2 - Written\"\n",
    "\n",
    "**Total:** 100 points  \n",
    "**Do not add extra imports unless explicitly allowed.**\n",
    "\n",
    "**⚠️ IMPORTANT FOR AUTOGRADING:**  \n",
    "Throughout this notebook, you will see comment annotations like `# Q1.1`, `# Q1.2.1`, `# Q2.1`, etc. **DO NOT remove or modify these annotations.** They are used by the autograder to identify and extract your answers for grading. Removing or changing them may result in your work not being graded correctly.\n",
    "\n",
    "**Notes**: \n",
    "Although not required, we encourage you to choose your own neural network hyperparameters to practice setting up and training MLPs! \n",
    "Some suggestions that you can try to tune:\n",
    "\n",
    "- **Model architecture**\n",
    "  - `hidden_dim` \n",
    "  - number of hidden layers \n",
    "  - activation function (e.g., ReLU)\n",
    "  - `dropout` rate \n",
    "\n",
    "- **Training settings**\n",
    "  - learning rate \n",
    "  - optimizer \n",
    "  - batch size \n",
    "  - number of epochs \n",
    "  - weight decay / L2 regularization\n",
    "\n",
    "\n",
    "If you want to explore other pretrained word embeddings, You can check the full list of available open embeddings in the `gensim` library [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models).\n",
    "\n",
    "Different pretrained embeddings have different vector dimensions (for example, 50, 100, 200, or 300).  Make sure to read the embedding dimension using `wv.vector_size`, and update any code that depends on this value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bda20d",
   "metadata": {},
   "source": [
    "## Setup / Imports\n",
    "\n",
    "You will be working on this assignment on Google Colab unless you have local GPUs.\n",
    "Run the cell below.  \n",
    "If you see errors about missing packages, install them in your environment (or follow the README.md associated with this assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ced96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone (download) the course repository from GitHub into this Colab session.\n",
    "# This gives us access to helper files like utils.py that the notebook depends on.\n",
    "!git clone https://github.com/uchicago-nlp-course/winter2026-assignments.git\n",
    "\n",
    "# Move into the homework directory so Python can find and import the helper files.\n",
    "%cd winter2026-assignments/A2\n",
    "\n",
    "# alternatively, you can create a folder in google with hw2_dist.ipynb, open hw2_dist.ipynb from that folder, and make sure that classification_util.py and word2vec_util.py are in the running folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required packages\n",
    "!pip install gensim nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a853ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info[0] == 3 and sys.version_info[1] >= 8\n",
    "\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Data and ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# NLP\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local utilities\n",
    "from classification_util import (\n",
    "    PAD,\n",
    "    load_sst2_splits,\n",
    "    preprocess_sst2,\n",
    "    load_snli_splits,\n",
    "    preprocess_snli,\n",
    "    load_with_retries,\n",
    "    get_embedding_matrix_and_word2idx,\n",
    "    SentenceIdDataset,\n",
    "    SNLIPairIdDataset,\n",
    "    eval_acc,\n",
    "    train_dan,\n",
    ")\n",
    "from word2vec_util import (\n",
    "    download_analogy_dataset,\n",
    "    load_analogy_dataset,\n",
    "    evaluate_analogies_gensim,\n",
    "    summarize_results,\n",
    ")\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3096",
   "metadata": {},
   "source": [
    "## Provided functions\n",
    "We implemented the train, eval_acc, and load_with_retries functions for you. Please read it carefully to understand what it is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13145de6",
   "metadata": {},
   "source": [
    "# Problem 1: Text Classification with SST (50 points)\n",
    "\n",
    "In this part of the assignment, you will study text classification using two common types of text representations: \n",
    "- Count-based methods that represent words by how frequently they appear with other words in a corpus. In this part, you will implement Bag-of-Words (BoW) matrix.\n",
    "- Dense, low-dimensional vector representations of words that capture semantic and syntactic relationships, such as Word2Vec and GloVe.\n",
    "\n",
    "## Sentiment Analysis \n",
    "\n",
    "Sentiment analysis is a natural language understanding task in which a model determines whether a sentence expresses a **positive** or **negative** opinion. Please refer to [lecture 3](https://uchicago-nlp-course.github.io/winter2026-lectures/lecture-03/#/2/1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sst2-load-markdown-5",
   "metadata": {},
   "source": [
    "## Load SST-2 dataset\n",
    "\n",
    "The SST-2 dataset is a widely used benchmark for this task. It consists of 215,154 short sentences taken from movie reviews, containinh individual sentences paired with sentiment labels:\n",
    "- **negative (0)**\n",
    "- **positive (1)**\n",
    "\n",
    "For example:\n",
    "\n",
    "| Sentence | Label |\n",
    "|---------|-------|\n",
    "| The movie was thrilling and engaging. | Positive |\n",
    "| The plot was dull and predictable. | Negative |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0j704vxlfyhr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_sst2_splits is imported from classification_util\n",
    "train_data, val_data, test_data = load_sst2_splits()\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "X_train, y_train = preprocess_sst2(train_data)\n",
    "X_val, y_val = preprocess_sst2(val_data)\n",
    "X_test, y_test = preprocess_sst2(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31567579",
   "metadata": {},
   "source": [
    "## Question 1.1 - BoW + Logistic Regression on SST2 [code] (15 points)\n",
    "\n",
    "In this part, we implement a Bag-of-Words (BoW) baseline for the Sentiment Analysis task. You will convert text to a BoW feature vector that captures word occurrence counts. We then train a logistic regression classifier to predict the Sentiment labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6e986",
   "metadata": {},
   "source": [
    "###  Build BoW Matrix\n",
    "To keep it simple, you will use `CountVectorizer` function from `scikit-learn` ([CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)). `CountVectorizer` can automatically converts each example into a vector of **word counts**. The result is a sparse matrix of shape **(num_examples, vocab_size)**.\n",
    "\n",
    "You will need to implement `CountVectorizer` on the three data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c986494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1.bow\n",
    "print(\"Vectorizing text...\")\n",
    "# TODO: Write your implementation here.\n",
    "# Use CountVectorizer to transform text data\n",
    "# - Fit on training data with min_df=5\n",
    "# - Transform train, test, and validation sets\n",
    "\n",
    "\n",
    "# Check shapes (X_train should have 53879 rows)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9c95e",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "In this step, we train a **logistic regression** classifier on the Bag-of-Words representations to perform Sentiment Analysis. Logistic regression is a strong linear baseline for text classification and allows us to evaluate how well simple word-count features capture sentiment relationships. \n",
    "\n",
    "Using the trained model, we evaluate performance on the test split and report **overall accuracy**.\n",
    "\n",
    "**Note**: Feel free to use the validation dataset for hyperparameter tuning!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "\n",
    "\n",
    "for c_value in [0.01, 0.1, 1.0, 10.0]:\n",
    "    # Train on training data\n",
    "    model = LogisticRegression(C=c_value, max_iter=20)\n",
    "    # TODO: Write your implementation here.\n",
    "    # Train a logistic regression model\n",
    "    # Use validation set for hyperparameter tuning (try different C values)\n",
    "    \n",
    "    \n",
    "# Report best model's test accuracy\n",
    "print(f\"Winner! Best C is {best_c}\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluation\n",
    "# Q1.1\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239be377",
   "metadata": {},
   "source": [
    "## Question 1.2 - DAN model with Open Pretrained Vectors on SST2 [code] (35 points)\n",
    "\n",
    "In this part, you will perform text classification using pretrained (\"open\") word embeddings and a small neural network classifier.\n",
    "\n",
    "Training high-quality word embeddings requires very large datasets and significant compute. For convenience and reproducibility, we will instead use open, pretrained embeddings through the `gensim` library. (e.g., GloVe trained on Wikipedia and Gigaword). These embeddings already capture useful semantic information and allow us to focus on how embeddings are used in downstream models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9630e9",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "We referenced the model the model structure of the paper: [Deep Unordered Composition Rivals Syntactic Methods for Text Classification](https://aclanthology.org/P15-1162/) (Iyyer et al., 2015)\n",
    "\n",
    "The combined averaged embedding feature vector is passed through a **multi-layer perceptron (MLP)**, followed by a **2-way softmax** layer that predicts one of the three NLI labels: **Positive** or **Negative**.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "This model architecture has already been introduced in lecture 3. Please refer to the [lecture slides](https://uchicago-nlp-course.github.io/winter2026-lectures/lecture-03/) for a detailed explanation of the model and its motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bed83",
   "metadata": {},
   "source": [
    "### Prepare data \n",
    "For this homework, we will use `gensim.downloader` to load in the `glove-wiki-gigaword-100` embedding, which is pretrained GloVe word embedding model trained on a large corpus combining Wikipedia and Gigaword news text. It represents each word as a 100-dimensional ($\\text{DIM}$=100) vector learned from global word co-occurrence statistics, so that words with similar meanings have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570417fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_with_retries is imported from classification_util\n",
    "wv = load_with_retries(\"glove-wiki-gigaword-100\")\n",
    "DIM = wv.vector_size\n",
    "print(\"Loaded vectors:\", DIM, \"dim | vocab:\", len(wv))\n",
    "embedding_matrix, word2idx = get_embedding_matrix_and_word2idx(wv)\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1fbfd",
   "metadata": {},
   "source": [
    "### Question 1.2.1: Dynamic padding with a collate function (10 points)\n",
    "\n",
    "You will now implement a collate function to dynamically pad variable-length sentences so they can be batched together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PAD token as 0\n",
    "PAD = 0\n",
    "\n",
    "def collate_pad(batch):\n",
    "    \"\"\"\n",
    "    Collate function for single-sentence datasets with dynamic padding.\n",
    "\n",
    "    This collate function is responsible for dynamically padding sentences within\n",
    "    each batch so they can be processed together by the model. Each dataset example\n",
    "    returns a variable-length sequence of word IDs, since sentences naturally have\n",
    "    different lengths. When forming a batch, collate_pad takes all sentence ID\n",
    "    sequences in the batch and pads the shorter ones with the PAD token so that\n",
    "    every sentence has the same length as the longest sentence in that batch.\n",
    "    Padding is applied only at the batch level, which is more efficient than\n",
    "    padding all sentences to a global maximum length. The function also stacks the\n",
    "    labels into a single tensor. As a result, the model receives a rectangular\n",
    "    tensor of shape (batch_size, max_sentence_length_in_batch) along with the\n",
    "    corresponding labels, while ensuring that padding tokens do not represent\n",
    "    real words.\n",
    "\n",
    "    For more information, see:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
    "\n",
    "    Input:\n",
    "        batch: list of (ids, label) tuples\n",
    "    Output:\n",
    "        ids_padded: LongTensor of shape (B, T_max)\n",
    "        labels: LongTensor of shape (B,)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the collate function.\n",
    "    # 1. Separate the batch into a list of token ID tensors and a list of labels\n",
    "    # 2. Pad the token ID sequences so they all have the same length\n",
    "    #    - Use pad_sequence\n",
    "    # 3. Stack labels into a single tensor\n",
    "    # 4. Return the padded token IDs and labels\n",
    "\n",
    "    return ids_padded, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fb8fa",
   "metadata": {},
   "source": [
    "### Testing the collate_pad function\n",
    "\n",
    "Before using `collate_pad` from `classificatio_utils.py`, let's verify it works correctly on simple examples. This function pads sentences to the same length within a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2.1\n",
    "def test_collate_pad_sst():\n",
    "    \"\"\"\n",
    "    Test collate_pad with simple SST-style examples (single sentence).\n",
    "\n",
    "    Expected behavior:\n",
    "    - Sequences should be padded to max length in the batch\n",
    "    - Labels should be stacked into a tensor\n",
    "    - PAD token (0) should be used for padding\n",
    "    \"\"\"\n",
    "    # Create simple test batch with variable-length sequences\n",
    "    # Format: (ids, label)\n",
    "    batch = [\n",
    "        (torch.tensor([1, 2, 3]), torch.tensor(0)),          # len=3\n",
    "        (torch.tensor([4, 5]), torch.tensor(1)),             # len=2\n",
    "        (torch.tensor([6, 7, 8, 9]), torch.tensor(0)),       # len=4  (max)\n",
    "    ]\n",
    "\n",
    "    ids_padded, labels = collate_pad(batch)\n",
    "\n",
    "    # Check shapes\n",
    "    assert ids_padded.shape == (3, 4), f\"Expected ids shape (3, 4), got {ids_padded.shape}\"\n",
    "    assert labels.shape == (3,), f\"Expected labels shape (3,), got {labels.shape}\"\n",
    "\n",
    "    # Check padding values (PAD=0)\n",
    "    # First: [1,2,3] -> [1,2,3,0]\n",
    "    assert ids_padded[0].tolist() == [1, 2, 3, 0], f\"First example incorrect: {ids_padded[0].tolist()}\"\n",
    "    # Second: [4,5] -> [4,5,0,0]\n",
    "    assert ids_padded[1].tolist() == [4, 5, 0, 0], f\"Second example incorrect: {ids_padded[1].tolist()}\"\n",
    "    # Third: [6,7,8,9] -> no padding\n",
    "    assert ids_padded[2].tolist() == [6, 7, 8, 9], f\"Third example incorrect: {ids_padded[2].tolist()}\"\n",
    "\n",
    "    # Check labels\n",
    "    assert labels.tolist() == [0, 1, 0], f\"Labels incorrect: {labels.tolist()}\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "    print(f\"ids_padded shape: {ids_padded.shape}\")\n",
    "    print(f\"labels: {labels}\")\n",
    "\n",
    "# Run the test\n",
    "test_collate_pad_sst()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe1abc",
   "metadata": {},
   "source": [
    "### Question 1.2.2 Tokenization (5 points)\n",
    "In this part, you will write a function `tokens_to_ids` that converts tokens into their corresponding word IDs and enforces a maximum sequence length. The helper function below will later be called by `SentenceIdDataset` to map tokenized sentences to model-ready word ID sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2.2\n",
    "def tokens_to_ids(tokens, word2idx, max_len=50):\n",
    "    \"\"\"Convert tokens to IDs, truncating to max_len.\"\"\"\n",
    "    # TODO: Convert tokens to word IDs.\n",
    "    # 1. Iterate over the input tokens\n",
    "    # 2. Truncate the sequence to max_len tokens\n",
    "    # 3. Return the list of token IDs\n",
    "\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846557b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for SST-2\n",
    "train_ds = SentenceIdDataset(train_data, word2idx, tokens_to_ids, max_len=50)\n",
    "val_ds   = SentenceIdDataset(val_data, word2idx, tokens_to_ids, max_len=50)\n",
    "test_ds  = SentenceIdDataset(test_data, word2idx, tokens_to_ids, max_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True,  collate_fn=collate_pad)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False, collate_fn=collate_pad)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collate_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cee0fa",
   "metadata": {},
   "source": [
    "### Problem 1.2.3 Training and evaluation (15 points)\n",
    "\n",
    "In this part, you will need to finish the `SSTDANClassifier` model and train **two versions of the model**:\n",
    "\n",
    "- **Frozen embeddings** (`freeze=True`):  \n",
    "  The pretrained word embeddings are kept fixed, and only the classifier parameters are updated.\n",
    "\n",
    "- **Trainable embeddings** (`freeze=False`):  \n",
    "  The pretrained word embeddings are fine-tuned jointly with the classifier.\n",
    "\n",
    "You should report results for **both settings** and briefly compare their performance.\n",
    "\n",
    "The model is trained using the **AdamW** optimizer and **cross-entropy loss**.\n",
    "\n",
    "**Training settings**:\n",
    "- **Learning rate**: `3e-4`\n",
    "- **Weight decay**: `1e-4`\n",
    "- **Epochs**: `10`\n",
    "\n",
    "After training, report the **test accuracy**.\n",
    "\n",
    "**Note**: The trainable version would take about 6 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDANClassifier(nn.Module):\n",
    "    \"\"\"DAN classifier for single-sentence classification (SST-2).\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, freeze=True, num_classes=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_matrix: numpy array of shape (V+1, D) with pretrained embeddings\n",
    "            freeze: if True, embeddings are not updated during training\n",
    "            num_classes: number of output classes (2 for SST)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Write your implementation here.\n",
    "        # 1. Create self.embedding using nn.Embedding.from_pretrained\n",
    "        # 2. Create self.mlp as an nn.Sequential with:\n",
    "        #    - 3 hidden layers of size 256 with ReLU and Dropout(0.2)\n",
    "        #    - Final linear layer to num_classes\n",
    "        pass\n",
    "\n",
    "    def masked_avg(self, ids):\n",
    "        \"\"\"\n",
    "        Compute average embedding for a batch of sentences, ignoring PAD tokens.\n",
    "        \n",
    "        Args:\n",
    "            ids: LongTensor of shape (B, T)\n",
    "        Returns:\n",
    "            Tensor of shape (B, D) - averaged embeddings\n",
    "        \"\"\"\n",
    "        # TODO: Write your implementation here.\n",
    "        # 1. Look up embeddings: emb = self.embedding(ids)  -> (B, T, D)\n",
    "        # 2. Create mask for non-PAD tokens: mask = (ids != PAD)\n",
    "        # 3. Zero out PAD embeddings\n",
    "        # 4. Sum embeddings and divide by number of non-PAD tokens\n",
    "        pass\n",
    "\n",
    "    def forward(self, ids):\n",
    "        \"\"\"\n",
    "        Forward pass for single-sentence classification.\n",
    "        \n",
    "        Args:\n",
    "            ids: LongTensor of shape (B, T) - sentence token IDs\n",
    "        Returns:\n",
    "            logits: Tensor of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # TODO: Write your implementation here.\n",
    "        # 1. Get averaged sentence embedding using masked_avg\n",
    "        # 2. Pass through MLP\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.1 Train + Eval (Freeze embeddings)\n",
    "\n",
    "model_frozen = # define the model\n",
    "model_frozen = # call train_dan with appropriate parameters\n",
    "accuracy = # call eval_acc with appropriate parameters\n",
    "\n",
    "# Q1.2.a\n",
    "print(\"test acc with frozen embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c32a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 TODO: Train + Eval (Trainable embeddings)\n",
    "model_trainable = # define the model\n",
    "model_trainable = # call train_dan with appropriate parameters\n",
    "accuracy = # call eval_acc with appropriate parameters\n",
    "\n",
    "# Q1.2.b\n",
    "print(\"test acc with trainable embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a892",
   "metadata": {},
   "source": [
    "## Question 1.3 - Result interpretation - SST2 [written] (5 points)\n",
    "\n",
    "Compare the accuracy results across different approaches and explain your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b9db6",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c48f437",
   "metadata": {},
   "source": [
    "## Problem 2: Natural Language Inference (30 points)\n",
    "NLI evaluates a model's ability to understand sentence meaning, reasoning, and semantic relationships, making it a core benchmark for assessing natural language understanding. Please refer to [lecture 3](https://uchicago-nlp-course.github.io/winter2026-lectures/lecture-03/#/2/3) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7759d",
   "metadata": {},
   "source": [
    "## Load SNLI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2443b",
   "metadata": {},
   "source": [
    "The [SNLI](https://huggingface.co/datasets/stanfordnlp/snli) corpus is a collection of 570k human-written English sentence pairs that is designed for natural language inference (NLI), also known as textual entailment. If you'd like to explore NLI in more depth, we encourage you to check out [this original SNLI paper](https://arxiv.org/abs/1508.05326) for intuitive explanations and experiments of NLI.\n",
    "\n",
    "SNLI has sentence pairs `(premise, hypothesis)` with labels:\n",
    "- `entailment  (0)` \n",
    "- `contradiction (2)`\n",
    "- `neutral (1)`\n",
    "\n",
    "We load SNLI with predefined **train**, **validation**, and **test** splits. We then filter out invalid examples by removing rows with missing sentences or unlabeled instances (where `label == -1`). For consistency in preprocessing, we convert both the premise and hypothesis text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_snli_splits is imported from classification_util\n",
    "train_data, val_data, test_data = load_snli_splits()\n",
    "print(len(train_data), len(val_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678702a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][\"premise\"])\n",
    "print(train_data[0][\"hypothesis\"])\n",
    "print(train_data[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba99e6",
   "metadata": {},
   "source": [
    "## Question 2.1 - BoW + Logistic Regression on SNLI [code] (10 points)\n",
    "\n",
    "In this part, You will use the same method in Question 1.1 and implement it on Natural Language Inference(NLI). This model will be serve as a baseline model for this NLI task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872702a5",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Unlike sentiment analysis, the prediction depends on the relationship between two sentences in natural language inference (NLI), not on either sentence alone. To allow a single text classifier to consider information from both the premise and the hypothesis, we provided a `preprocess_snli` function to concatenate them into one combined input string. The separator token (`[SEP]`) marks the boundary between the two sentences, helping preserve their roles while keeping the model architecture simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_snli is imported from classification_util\n",
    "# It concatenates premise and hypothesis with [SEP] separator\n",
    "X_train_raw, y_train = preprocess_snli(train_data) \n",
    "X_test_raw, y_test = preprocess_snli(test_data)\n",
    "X_val_raw, y_val = preprocess_snli(val_data)\n",
    "\n",
    "print(X_train_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e095ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1.bow\n",
    "print(\"Vectorizing text...\")\n",
    "# TODO: Write your implementation here.\n",
    "# Same approach as SST2\n",
    "\n",
    "\n",
    "# Check shapes (X_train should have 549367 rows)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "# TODO: Write your implementation here.\n",
    "# Same approach as SST2\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "# Q2.1\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['entailment', 'neutral', 'contradiction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60faced4",
   "metadata": {},
   "source": [
    "## Question 2.2 - GloVe + MLP classifier for SNLI [code+written] (15 points)\n",
    "\n",
    "In this part, In this part, you will repeat the **averaged word embeddings + MLP (DAN-style)** classification pipeline from the previous section using the same fixed embedding `glove-wiki-gigaword-100`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b71f7",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "Unlike SST2, you will need to feed multiple sentences into the neural network. In this part, our model follows a Siamese-style sentence pair architecture, where the premise and hypothesis are processed independently using the same encoder. In addition to the first paper (Iyyer et al., 2015), We referenced the model structure of the following paper:\n",
    "- [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364) (Conneau et al., 2017)\n",
    "\n",
    "The first part is the same, each sentence is converted into a fixed-length vector by averaging pretrained word embeddings (Iyyer et al., 2015).\n",
    "\n",
    "Since the is a NLI problem, you will instead obtain:\n",
    "- $p$: averaged embedding for the premise\n",
    "- $h$: averaged embedding for the hypothesis\n",
    "\n",
    "After encoding the premise and hypothesis into vectors $\\mathbf{p}$ and $\\mathbf{h}$, we explicitly model their relationship by constructing a **comparison vector**(Conneau et al., 2017):\n",
    "$$\n",
    "\\left[ \\mathbf{p},\\; \\mathbf{h},\\; |\\mathbf{p} - \\mathbf{h}|,\\; \\mathbf{p} \\odot \\mathbf{h} \\right]\n",
    "$$\n",
    "\n",
    "Here, the absolute difference $|\\mathbf{p} - \\mathbf{h}|$ captures how far the two sentences are **semantically apart** in a symmetric way, while the element-wise product $\\mathbf{p} \\odot \\mathbf{h}$ captures **feature overlap and similarity** between them. These comparison features are widely used in natural language inference models because they make it easier for the classifier to reason about sentence relationships.\n",
    "\n",
    "The combined feature vector is then passed through a **multi-layer perceptron (MLP)**, followed by a **3-way softmax** layer that predicts one of the three NLI labels: **entailment**, **neutral**, or **contradiction**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080b453",
   "metadata": {},
   "source": [
    "### Question 2.2.1: Dynamic padding with a collate function (5 points)\n",
    "\n",
    "Just like in the SST problem, you will implement a collate function for sentence-pair SNLI dataset.\n",
    "The function should dynamically pad premise and hypothesis sequences within each batch and return padded tensors along with the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_pair_pad(batch):\n",
    "    \"\"\"\n",
    "    Collate function for sentence-pair datasets (SNLI) with dynamic padding.\n",
    "\n",
    "    Input:\n",
    "        batch: list of (premise_ids, hypothesis_ids, label) tuples\n",
    "    Output:\n",
    "        p_padded: LongTensor of shape (B, Tp_max)\n",
    "        h_padded: LongTensor of shape (B, Th_max)\n",
    "        labels: LongTensor of shape (B,)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the collate function for sentence pairs.\n",
    "    # 1. Separate the batch into premise IDs, hypothesis IDs, and labels\n",
    "    # 2. Pad premise ID sequences to the max premise length in the batch\n",
    "    # 3. Pad hypothesis ID sequences to the max hypothesis length in the batch\n",
    "    # 5. Return padded premises, padded hypotheses, and labels\n",
    "\n",
    "    return p_padded, h_padded, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf4daf",
   "metadata": {},
   "source": [
    "### Testing the collate_pair_pad function\n",
    "\n",
    "Before using `collate_pair_pad` from `utils.py`, let's verify it works correctly on simple examples. This function pads premise-hypothesis pairs to the same length within a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.2.1\n",
    "def test_collate_pair_pad():\n",
    "    \"\"\"\n",
    "    Test collate_pair_pad with simple examples.\n",
    "    \n",
    "    Expected behavior:\n",
    "    - Premises should be padded to max premise length in batch\n",
    "    - Hypotheses should be padded to max hypothesis length in batch\n",
    "    - Labels should be stacked into a tensor\n",
    "    - PAD token (0) should be used for padding\n",
    "    \"\"\"\n",
    "    # Create simple test batch with variable-length sequences\n",
    "    # Format: (premise_ids, hypothesis_ids, label)\n",
    "    batch = [\n",
    "        (torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor(0)),        # premise len=3, hyp len=2\n",
    "        (torch.tensor([6, 7]), torch.tensor([8, 9, 10, 11]), torch.tensor(1)),   # premise len=2, hyp len=4\n",
    "        (torch.tensor([12]), torch.tensor([13, 14, 15]), torch.tensor(2)),       # premise len=1, hyp len=3\n",
    "    ]\n",
    "    \n",
    "    p_padded, h_padded, labels = collate_pair_pad(batch)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert p_padded.shape == (3, 3), f\"Expected premise shape (3, 3), got {p_padded.shape}\"\n",
    "    assert h_padded.shape == (3, 4), f\"Expected hypothesis shape (3, 4), got {h_padded.shape}\"\n",
    "    assert labels.shape == (3,), f\"Expected labels shape (3,), got {labels.shape}\"\n",
    "    \n",
    "    # Check padding values (PAD=0)\n",
    "    # First premise [1,2,3] should have no padding\n",
    "    assert p_padded[0].tolist() == [1, 2, 3], f\"First premise incorrect: {p_padded[0].tolist()}\"\n",
    "    # Second premise [6,7] should be padded to [6,7,0]\n",
    "    assert p_padded[1].tolist() == [6, 7, 0], f\"Second premise incorrect: {p_padded[1].tolist()}\"\n",
    "    # Third premise [12] should be padded to [12,0,0]\n",
    "    assert p_padded[2].tolist() == [12, 0, 0], f\"Third premise incorrect: {p_padded[2].tolist()}\"\n",
    "    \n",
    "    # Check hypothesis padding\n",
    "    # First hypothesis [4,5] should be padded to [4,5,0,0]\n",
    "    assert h_padded[0].tolist() == [4, 5, 0, 0], f\"First hypothesis incorrect: {h_padded[0].tolist()}\"\n",
    "    # Second hypothesis [8,9,10,11] should have no padding\n",
    "    assert h_padded[1].tolist() == [8, 9, 10, 11], f\"Second hypothesis incorrect: {h_padded[1].tolist()}\"\n",
    "    # Third hypothesis [13,14,15] should be padded to [13,14,15,0]\n",
    "    assert h_padded[2].tolist() == [13, 14, 15, 0], f\"Third hypothesis incorrect: {h_padded[2].tolist()}\"\n",
    "    \n",
    "    # Check labels\n",
    "    assert labels.tolist() == [0, 1, 2], f\"Labels incorrect: {labels.tolist()}\"\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    print(f\"Premise padded shape: {p_padded.shape}\")\n",
    "    print(f\"Hypothesis padded shape: {h_padded.shape}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "\n",
    "# Run the test\n",
    "test_collate_pair_pad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049e0ba",
   "metadata": {},
   "source": [
    "### SNLIDANClassifier: Extending SSTDANClassifier for sentence pairs (10 points)\n",
    "\n",
    "For SNLI, we need to process **two sentences** (premise and hypothesis) instead of one. The `SNLIDANClassifier` extends `SSTDANClassifier` by:\n",
    "\n",
    "1. **Inheriting** the `masked_avg` method from the parent class (no need to reimplement!)\n",
    "2. **Overriding** `__init__` to change the MLP input dimension to `4 * D` (for the comparison vector)\n",
    "3. **Overriding** `forward` to:\n",
    "   - Compute averaged embeddings for both premise ($p$) and hypothesis ($h$)\n",
    "   - Construct the comparison vector: $[p, h, |p-h|, p \\odot h]$\n",
    "   - Pass through the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDANClassifier(SSTDANClassifier):\n",
    "    \"\"\"DAN classifier for sentence-pair classification (SNLI).\n",
    "    \n",
    "    Inherits masked_avg from SSTDANClassifier.\n",
    "    You need to implement __init__ and forward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, freeze=True, num_classes=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_matrix: numpy array of shape (V+1, D) with pretrained embeddings\n",
    "            freeze: if True, embeddings are not updated during training\n",
    "            num_classes: number of output classes (3 for SNLI)\n",
    "        \"\"\"\n",
    "        # NOTE: We call nn.Module.__init__ directly to avoid calling parent's __init__\n",
    "        # which would create an MLP with wrong input dimension\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        # TODO: Write your implementation here.\n",
    "        # 1. Create self.embedding using nn.Embedding.from_pretrained (same as SSTDANClassifier)\n",
    "        # 2. Create self.mlp with input dimension 4*D (for [u, v, |u-v|, u*v])\n",
    "        #    Same architecture: 3 hidden layers of 256 with ReLU and Dropout(0.2)\n",
    "        pass\n",
    "\n",
    "    def forward(self, p_ids, h_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for sentence-pair classification.\n",
    "        \n",
    "        Args:\n",
    "            p_ids: LongTensor of shape (B, Tp) - premise token IDs\n",
    "            h_ids: LongTensor of shape (B, Th) - hypothesis token IDs\n",
    "        Returns:\n",
    "            logits: Tensor of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # TODO: Write your implementation here.\n",
    "        # 1. Get averaged embeddings for premise (u) and hypothesis (v) using self.masked_avg\n",
    "        # 2. Construct comparison vector: [u, v, |u-v|, u*v]\n",
    "        # 3. Pass through MLP\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98228be",
   "metadata": {},
   "source": [
    "### Training a DAN-style MLP classifier on the SNLI averaged-embedding features\n",
    "In this part, you will repeat the **averaged word embeddings + MLP (DAN-style)** model structure from the previous section. \n",
    "You **do not** need to implement tokenization, dataset classes, or data loaders for this question.  \n",
    "These components are provided and are conceptually the same as in the SST experiments, extended to sentence pairs. For convenience, we will help you implement the tokenization part with helper functions from `utils.py`.\n",
    "\n",
    "For this part, you will use the `SNLIDANClassifier` and the shared `train_dan` function with `snli_mode=True`. Since the training takes a lot of time, you will only need to run 3 epochs for the trainable version.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "The training of this part may take up to 20 min on cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94020a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SNLIPairIdDataset(train_data, word2idx, tokens_to_ids, max_len=50)\n",
    "val_ds   = SNLIPairIdDataset(val_data,   word2idx, tokens_to_ids, max_len=50)\n",
    "test_ds  = SNLIPairIdDataset(test_data,  word2idx, tokens_to_ids,max_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True,  collate_fn=collate_pair_pad)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=512, shuffle=False, collate_fn=collate_pair_pad)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False, collate_fn=collate_pair_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1 Train + Eval (Freeze embeddings)\n",
    "model_frozen = # define the model\n",
    "model_frozen = # call train_dan with appropriate parameters\n",
    "accuracy = # call eval_acc with appropriate parameters\n",
    "\n",
    "# Q2.2.a\n",
    "print(\"test acc with frozen embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2 Train + Eval (Trainable embeddings)\n",
    "model_trainable = # define the model\n",
    "model_trainable = # call train_dan with appropriate parameters\n",
    "accuracy = # call eval_acc with appropriate parameters\n",
    "\n",
    "# Q2.2.b\n",
    "print(\"test acc with trainable embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da7097",
   "metadata": {},
   "source": [
    "## Question 2.3 - Result interpretation [written] (5 points)\n",
    "\n",
    "Which embedding type performed better (BoW vs Pretrained GloVe)? Why might pretrained GloVe outperform frequency-based embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29814c",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a50949",
   "metadata": {},
   "source": [
    "# Problem 3: Word2Vec (20 points)\n",
    "\n",
    "In this problem, you will study how the performance of Word2Vec embeddings depends on training data size and embedding dimensionality.\n",
    "\n",
    "You will train multiple Word2Vec models using the **skip-gram architecture**, varying only:\n",
    "- the number of training sentences, and\n",
    "- the dimensionality of the word vectors.\n",
    "\n",
    "All other hyperparameters should be kept fixed across models.  \n",
    "This experimental setup follows the analysis presented in the original [word2vec's original paper](https://arxiv.org/abs/1301.3781) (Mikolov et al., 2013), where different training settings are compared using word analogy evaluations(see the table below).\n",
    "\n",
    "![CBOW table](CBOW_table.png)\n",
    "\n",
    "To evaluate embedding quality, you will use the Google Semantic-Syntactic Word Analogy dataset.\n",
    "\n",
    "You will train a total of **9 models**. Results should be reported in a table following the format of Table 2, with rows corresponding to embedding dimensionality and columns corresponding to training data size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd4338",
   "metadata": {},
   "source": [
    "### Preparing sentences for Word2Vec\n",
    "We will train a word2vec model on the [Simple Wikipedia](https://huggingface.co/datasets/rahular/simple-wikipedia) dataset. This corpus contains 770k rows from a text-only dump of the Simple Wikipedia (English).\n",
    "\n",
    "In this section, we will convert the raw Simple Wikipedia text into a **list of tokenized sentences**, which is the expected input format for training Word2Vec.\n",
    "The output is a Python list where each element is one sentence represented as a list of tokens, e.g.\n",
    "`[['the', 'cat', 'sat', 'down'], ['it', 'was', 'tired'], ...]`. \n",
    "\n",
    "To do so, we will use `nltk`, a popular library for many text processing frameworks and resources. Read more [here](https://www.nltk.org/).\n",
    "\n",
    "Here we help you implement the `read_and_sentencize_corpus`, which can later be used to read and tokenize the Simple Wikipedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_sentencize_corpus(data):\n",
    "    \"\"\" Read files from the Simple Wikipedia dataset.\n",
    "\n",
    "        Return:\n",
    "            list of lists, with tokenized sentences from each of the processed rows\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    files = data[\"text\"]\n",
    "    for review in tqdm(files):\n",
    "        # Split reviews into sentences\n",
    "        sentences = sent_tokenize(review)\n",
    "        for sentence in sentences:\n",
    "            # Tokenize and preprocess each sentence\n",
    "            tokenized_sentences.append(word_tokenize(sentence.lower()))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f746d10",
   "metadata": {},
   "source": [
    "We'll now implement the training of a word2vec model on our sentences. word2vec embeddings convert words into dense vector representations that capture semantic relationships between them. Training the model involves analyzing the context in which words appear within sentences to learn these meaningful vector representations.\n",
    "\n",
    "This function trains a Word2Vec model with the following settings:\n",
    "- the training sentences provided via the input argument `sentences`\n",
    "- embedding dimensionality set by the input argument `dim`\n",
    "- context window size of 5\n",
    "- minimum word count of 5\n",
    "- skip-gram training algorithm\n",
    "\n",
    "All other parameters should be left at their default values.\n",
    "\n",
    "This function will be reused in later parts of the assignment to train **9 different Word2Vec models**, where both the **training corpus size (and resulting vocabulary)** and the **embedding dimensionality** are varied. Keeping the training logic encapsulated in a single function ensures consistency across experimental settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38914c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences, dim):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        sentences (List[List[str]]):\n",
    "            Subset of the tokenized training corpus. \n",
    "\n",
    "        dim (int):\n",
    "            Dimensionality of the learned word vectors.\n",
    "\n",
    "    Returns:\n",
    "        word_vectors (gensim.models.KeyedVectors):\n",
    "            Trained word vectors learned by Word2Vec. \n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=dim,\n",
    "        min_count=5,\n",
    "        window=5,\n",
    "        sg=1,  # Skip-gram\n",
    "        workers=4,\n",
    "    )\n",
    "    word_vectors = model.wv\n",
    "\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b1eae",
   "metadata": {},
   "source": [
    "## Question 3.1 - Train Word2Vec Scaling Grid [code] (10 points)\n",
    "\n",
    "In this question, you will implement a function that trains a **grid of Word2Vec models** across multiple training data sizes and embedding dimensionalities.\n",
    "\n",
    "You will evaluate the following parameter grid:\n",
    "- **Training sizes**: 100k, 200k, 400k sentences\n",
    "- **Embedding dimensions**: 50, 100, 300\n",
    "\n",
    "The goal is to systematically study how Word2Vec performance scales as:\n",
    "- the number of training sentences increases, and\n",
    "- the dimensionality of the word embeddings increases.\n",
    "\n",
    "The function `train_scaling_grid` should iterate over all combinations of the provided training sizes (`train_sizes`) and embedding dimensions (`embed_dims`). For each combination, it should:\n",
    "1. load a subset of the training corpus containing the specified number of sentences\n",
    "2. convert the raw text into tokenized sentences suitable for Word2Vec training\n",
    "3. train a Word2Vec model using the `train_word2vec` function implemented in the previous question, and\n",
    "4. store the trained word vectors in a dictionary indexed by `(number_of_sentences, embedding_dimension)`.\n",
    "\n",
    "The function should return a dictionary mapping each `(n_sentences, dim)` pair to its corresponding trained Word2Vec model.\n",
    "\n",
    "**Hints**:\n",
    "Use the provided `read_and_sentencize_corpus` to convert raw text into a list of tokenized sentences before training Word2Vec.\n",
    "\n",
    "**Note**:\n",
    "This may take around 20 minutes to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1\n",
    "train_sizes = [100000, 200000, 400000]\n",
    "embed_dims  = [50, 100, 300]\n",
    "\n",
    "max_n = max(train_sizes)\n",
    "#Load 400000 sentences from Simple Wikipedia\n",
    "ds = load_dataset(\"rahular/simple-wikipedia\", split=f\"train[:{max_n}]\")\n",
    "all_sents = read_and_sentencize_corpus(ds)\n",
    "\n",
    "def train_scaling_grid(train_sizes, embed_dims, base_seed=0):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        train_sizes (List[int]): \n",
    "            List of training corpus sizes, where each value specifies the number\n",
    "            of sentences to use for training a Word2Vec model.\n",
    "\n",
    "        embed_dims (List[int]): \n",
    "            List of embedding dimensionalities to use when training Word2Vec models.\n",
    "\n",
    "        base_seed (int): \n",
    "            Base random seed for reproducibility across different training runs.\n",
    "\n",
    "    Returns:\n",
    "        models (dict): \n",
    "            A dictionary mapping (n_sentences, dim) -> trained Word2Vec word vectors.\n",
    "            Each entry corresponds to a Word2Vec model trained using `n_sentences`\n",
    "            sentences and embedding dimensionality `dim`.\n",
    "        token_counts (dict):\n",
    "            A dictionary records the number of words in the training corpus for each size.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    token_counts = {}\n",
    "    # TODO: Write your implementation here.\n",
    "    # For each training size:\n",
    "    #   1. Load the first n sentences from Simple Wikipedia subset all_sents\n",
    "    #   2. Tokenize using read_and_sentencize_corpus\n",
    "    #   3. Count tokens\n",
    "    #   4. For each embedding dimension, train Word2Vec model\n",
    "    #   5. Store in models dict with key (n, dim)\n",
    "\n",
    "    return models, token_counts\n",
    "\n",
    "# Train models (this can take time; you can reduce train_sizes or epochs for debugging)\n",
    "models, word_counts = train_scaling_grid(train_sizes, embed_dims, base_seed=42)\n",
    "\n",
    "print(\"\\nTrained models:\", len(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3927e",
   "metadata": {},
   "source": [
    "## Question 3.2 Word Analogy Evaluation [written] (5 points)\n",
    "\n",
    "To evaluate the quality of the trained Word2Vec embeddings, we use the **Google Semantic-Syntactic Word Analogy dataset**, introduced by Mikolov et al. (2013).\n",
    "\n",
    "This dataset consists of analogy questions of the form:\n",
    "$$\n",
    "a : b :: c : d\n",
    "$$\n",
    "which can be read as *\"a is to b as c is to d.\"*  \n",
    "Each question tests whether word vectors encode linguistic regularities that can be recovered through vector arithmetic.\n",
    "\n",
    "To answer an analogy question $(a, b, c, d)$, the model computes the vector:\n",
    "$$\n",
    "\\mathbf{v}(b) - \\mathbf{v}(a) + \\mathbf{v}(c)\n",
    "$$\n",
    "and checks whether the word whose embedding is **closest in cosine similarity** to this result is the correct target word $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c64c76",
   "metadata": {},
   "source": [
    "All evaluation utilities have already been implemented for you in `utils.py`. In particular, the provided functions:\n",
    "- `download_analogy_dataset`\n",
    "- `load_analogy_dataset`\n",
    "- `evaluate_analogies_gensim`\n",
    "- `summarize_results`\n",
    "handle dataset loading, analogy evaluation, and result aggregation. You do not need to write any additional code in this section.\n",
    "\n",
    "The code below evaluates each trained Word2Vec model in `models` and computes analogy accuracy across different analogy categories. The results are then summarized for each `(number_of_sentences, embedding_dimension)` setting.\n",
    "\n",
    "Based on the printed outputs and summarized results, you need to write a short analysis on this experement. What results do you find interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load analogy dataset once\n",
    "path = download_analogy_dataset()              # downloads only if missing\n",
    "analogy_data = load_analogy_dataset(path)\n",
    "print(f\"Loaded {len(analogy_data)} analogy categories\")\n",
    "\n",
    "# Show total questions\n",
    "total = sum(len(q) for q in analogy_data.values())\n",
    "print(f\"Total questions: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Evaluate each trained model in MODELS\n",
    "# MODELS maps: (n_sentences, dim) -> trained KeyedVectors (wv)\n",
    "results = {}\n",
    "for key, wv in models.items():\n",
    "    # evaluate_analogies_gensim returns per-category stats (e.g., correct/total)\n",
    "    results[key] = evaluate_analogies_gensim(wv, analogy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Summarize per-category results into one overall accuracy per model\n",
    "all_results = {}\n",
    "for (n_sent, dim), per_cat_results in results.items():\n",
    "    name = f\"n={n_sent}_dim={dim}\"\n",
    "    # summarize_results returns a dict with 'overall_accuracy', totals, and per-category breakdown\n",
    "    all_results[(n_sent, dim)] = summarize_results(\n",
    "        per_cat_results,\n",
    "        name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Convert summarized results into a DataFrame for easy pivoting\n",
    "rows = []\n",
    "for (n_sent, dim), s in all_results.items():\n",
    "    rows.append({\n",
    "        \"dim\": dim,\n",
    "        \"n_sentences\": n_sent,\n",
    "        \"accuracy\": 100 * s[\"overall_accuracy\"],  \n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"n_words\"] = df[\"n_sentences\"].map(word_counts)\n",
    "\n",
    "# Q3.2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Write your answer here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 - Visualization of Word2Vec Scaling Behavior [code] (5 Points)\n",
    "\n",
    "In this question, you will visualize how Word2Vec analogy accuracy changes as a function of training data size and embedding dimensionality.\n",
    "\n",
    "Using the summarized results from the previous section, create a line plot where:\n",
    "- the x-axis represents the number of words(not sentences),\n",
    "- the y-axis represents analogy accuracy (in percentage),\n",
    "- each line corresponds to a different embedding dimensionality.\n",
    "\n",
    "This plot provides a visual summary of the scaling behavior observed in the Word2Vec experiments and complements the table reported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "# TODO: Write your implementation here.\n",
    "# Create a line plot showing:\n",
    "# - x-axis: number of training words\n",
    "# - y-axis: analogy accuracy (%)\n",
    "# - separate line for each embedding dimension\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of training words\")\n",
    "plt.ylabel(\"Analogy accuracy (%)\")\n",
    "plt.title(\"Word2Vec Analogy Accuracy vs Training Size\")\n",
    "plt.legend(title=\"Embedding dimension\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
