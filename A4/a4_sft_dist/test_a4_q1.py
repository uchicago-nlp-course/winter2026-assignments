"""
Data Collection for A4 Q1 (SFT Data Encoding)

Run this script to generate A4-Q1.json for Gradescope submission.
No grading is performed here - only data collection.

Usage:
    python test_a4_q1.py

This will create A4-Q1.json in the same directory.
Submit both data_utils.py and A4-Q1.json to Gradescope.
"""

import json
import math
import os
import tempfile
import traceback
from pathlib import Path
from typing import Any, Dict, List

import torch
from transformers import AutoTokenizer

from data_utils import (
    LMDatasetConfig,
    encode_function,
    load_and_prepare_train_dataset,
)


MODEL_NAME = "google/gemma-3-1b-it"

# Global dictionary to store test results
test_results: Dict[str, Any] = {}


# ==============================================================================
# REFERENCE VALUES (generated by running the solution with the real tokenizer)
# ==============================================================================

REFERENCE = {
    "single_basic": {
        "input_ids": [
            [2, 105, 2364, 107, 76857, 506, 2269, 6596, 2608, 2918, 684, 2918, 236764, 532, 2847, 506, 1626, 40155, 3890, 236761, 108, 21009, 236787, 2900, 563, 236743, 236778, 900, 236743, 236800, 236881, 106, 107, 105, 4368, 107, 7925, 236787, 3792, 236789, 236751, 1751, 2918, 684, 2918, 236761, 236743, 236778, 900, 236743, 236800, 578, 236743, 236810, 236761, 107, 17667, 3890, 236787, 236743, 236810, 236761, 106, 107, 1]
        ],
        "labels": [
            [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 236743, 236778, 900, 236743, 236800, 578, 236743, 236810, 236761, 107, 17667, 3890, 236787, 236743, 236810, 236761, 106, 107, 1]
        ],
        "attention_mask": [
            [1] * 65
        ],
    },
    "single_with_final_answer": {
        "input_ids": [
            [2, 105, 2364, 107, 76857, 506, 2269, 6596, 2608, 2918, 684, 2918, 236764, 532, 2847, 506, 1626, 40155, 3890, 236761, 108, 21009, 236787, 1637, 611, 735, 236743, 236770, 236771, 36157, 532, 9039, 236743, 236800, 236764, 1217, 1551, 659, 2378, 236881, 106, 107, 105, 4368, 107, 7925, 236787, 3792, 236789, 236751, 1751, 2918, 684, 2918, 236761, 236743, 236770, 236771, 753, 236743, 236800, 578, 236743, 236832, 236761, 107, 17667, 3890, 236787, 236743, 236832, 236761, 106, 107, 1]
        ],
        "labels": [
            [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 236743, 236770, 236771, 753, 236743, 236800, 578, 236743, 236832, 236761, 107, 17667, 3890, 236787, 236743, 236832, 236761, 106, 107, 1]
        ],
        "attention_mask": [
            [1] * 75
        ],
    },
    "batch_two_examples": {
        "input_ids": [
            [2, 105, 2364, 107, 76857, 506, 2269, 6596, 2608, 2918, 684, 2918, 236764, 532, 2847, 506, 1626, 40155, 3890, 236761, 108, 21009, 236787, 2900, 563, 236743, 236778, 900, 236743, 236800, 236881, 106, 107, 105, 4368, 107, 7925, 236787, 3792, 236789, 236751, 1751, 2918, 684, 2918, 236761, 236743, 236778, 900, 236743, 236800, 578, 236743, 236810, 236761, 107, 17667, 3890, 236787, 236743, 236810, 236761, 106, 107, 1],
            [2, 105, 2364, 107, 76857, 506, 2269, 6596, 2608, 2918, 684, 2918, 236764, 532, 2847, 506, 1626, 40155, 3890, 236761, 108, 21009, 236787, 1637, 611, 735, 236743, 236770, 236771, 36157, 532, 9039, 236743, 236800, 236764, 1217, 1551, 659, 2378, 236881, 106, 107, 105, 4368, 107, 7925, 236787, 3792, 236789, 236751, 1751, 2918, 684, 2918, 236761, 236743, 236770, 236771, 753, 236743, 236800, 578, 236743, 236832, 236761, 107, 17667, 3890, 236787, 236743, 236832, 236761, 106, 107, 1],
        ],
        "labels": [
            [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 236743, 236778, 900, 236743, 236800, 578, 236743, 236810, 236761, 107, 17667, 3890, 236787, 236743, 236810, 236761, 106, 107, 1],
            [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 236743, 236770, 236771, 753, 236743, 236800, 578, 236743, 236832, 236761, 107, 17667, 3890, 236787, 236743, 236832, 236761, 106, 107, 1],
        ],
        "attention_mask": [
            [1] * 65,
            [1] * 75,
        ],
    },
    "truncation": {
        "input_ids": [
            [2, 105, 2364, 107, 76857, 506, 2269, 6596, 2608, 2918, 684, 2918, 236764, 532, 2847, 506, 1626, 40155, 3890, 236761, 108, 21009, 236787, 2900, 563, 236743, 236778, 900, 236743, 236800, 236881, 106, 107, 105, 4368, 107, 7925, 236787, 3792, 236789, 236751, 1751, 2918, 684, 2918, 236761, 236743, 236778, 900, 236743]
        ],
        "labels": [
            [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 236743, 236778, 900, 236743]
        ],
        "attention_mask": [
            [1] * 50
        ],
    },
}


# ==============================================================================
# TEST CASE DEFINITIONS
# ==============================================================================

ENCODE_TEST_CASES = {
    "single_basic": {
        "examples": {
            "question": ["What is 2 + 3?"],
            "answer": ["2 + 3 = 5. #### 5"],
        },
        "max_seq_length": None,
    },
    "single_with_final_answer": {
        "examples": {
            "question": ["If you have 10 apples and eat 3, how many are left?"],
            "answer": ["10 - 3 = 7. #### 7"],
            "final_answer": ["7"],
        },
        "max_seq_length": None,
    },
    "batch_two_examples": {
        "examples": {
            "question": [
                "What is 2 + 3?",
                "If you have 10 apples and eat 3, how many are left?",
            ],
            "answer": [
                "2 + 3 = 5. #### 5",
                "10 - 3 = 7. #### 7",
            ],
            "final_answer": [
                None,
                "7",
            ],
        },
        "max_seq_length": None,
    },
    "truncation": {
        "examples": {
            "question": ["What is 2 + 3?"],
            "answer": ["2 + 3 = 5. #### 5"],
        },
        "max_seq_length": 50,
    },
}

TRAIN_DATA_FILE = os.path.join(os.path.dirname(__file__), "data", "gsm_symbolic_train_4500_student.jsonl")
DATASET_PERCENTAGE = 0.01
DATASET_SEED = 42


# ==============================================================================
# DATA COLLECTION FUNCTIONS
# ==============================================================================

def collect_encode_function_data(tokenizer):
    """Collect data about encode_function implementation."""
    result: Dict[str, Any] = {
        "function_name": "encode_function",
        "test_cases": [],
        "error": None,
        "traceback": None,
    }

    try:
        for name, tc in ENCODE_TEST_CASES.items():
            tc_result: Dict[str, Any] = {"name": name, "error": None}
            try:
                output = encode_function(
                    examples=tc["examples"],
                    tokenizer=tokenizer,
                    max_seq_length=tc["max_seq_length"],
                )
                # Store student output as plain lists
                tc_result["input_ids"] = [
                    ids if isinstance(ids, list) else ids.tolist()
                    for ids in output["input_ids"]
                ]
                tc_result["labels"] = [
                    lbl if isinstance(lbl, list) else lbl.tolist()
                    for lbl in output["labels"]
                ]
                tc_result["attention_mask"] = [
                    am if isinstance(am, list) else am.tolist()
                    for am in output["attention_mask"]
                ]

                # Store reference for grading comparison
                ref = REFERENCE[name]
                tc_result["reference_input_ids"] = ref["input_ids"]
                tc_result["reference_labels"] = ref["labels"]
                tc_result["reference_attention_mask"] = ref["attention_mask"]

            except Exception as e:
                tc_result["error"] = str(e)
                tc_result["traceback"] = traceback.format_exc()

            result["test_cases"].append(tc_result)

    except Exception as e:
        result["error"] = str(e)
        result["traceback"] = traceback.format_exc()

    test_results["encode_function"] = result


def collect_load_and_prepare_data(tokenizer):
    """Collect data about load_and_prepare_train_dataset implementation."""
    result: Dict[str, Any] = {
        "function_name": "load_and_prepare_train_dataset",
        "test_cases": [],
        "error": None,
        "traceback": None,
    }

    try:
        cfg = LMDatasetConfig(
            train_file=TRAIN_DATA_FILE,
            max_seq_length=512,
            overwrite_cache=True,
            preprocessing_num_workers=None,
            sample_data_seed=DATASET_SEED,
            percentage=DATASET_PERCENTAGE,
        )

        dataset = load_and_prepare_train_dataset(tokenizer, cfg)

        # Compute expected row count from the raw data
        from datasets import load_dataset as _ld
        _raw = _ld("json", data_files={"train": cfg.train_file}, cache_dir=os.getenv("DATA_CACHE_DIR"))["train"]
        expected_rows = max(1, int(math.floor(len(_raw) * DATASET_PERCENTAGE)))

        tc_result: Dict[str, Any] = {
            "name": "dataset_basic",
            "error": None,
            "column_names": list(dataset.column_names),
            "num_rows": len(dataset),
            "expected_rows": expected_rows,
        }

        # Check format: sample first row
        sample = dataset[0]
        tc_result["sample_keys"] = sorted(sample.keys())

        # Check if values are tensors
        type_checks: Dict[str, str] = {}
        for key in ["input_ids", "labels", "attention_mask"]:
            if key in sample:
                type_checks[key] = type(sample[key]).__name__
        tc_result["value_types"] = type_checks

        # Check structural validity of a sample
        if "input_ids" in sample and "labels" in sample and "attention_mask" in sample:
            ids = sample["input_ids"]
            lbls = sample["labels"]
            am = sample["attention_mask"]
            ids_len = len(ids) if hasattr(ids, '__len__') else ids.numel()
            lbls_len = len(lbls) if hasattr(lbls, '__len__') else lbls.numel()
            am_len = len(am) if hasattr(am, '__len__') else am.numel()
            tc_result["sample_lengths_match"] = (ids_len == lbls_len == am_len)
            tc_result["sample_length"] = ids_len

        result["test_cases"].append(tc_result)

    except Exception as e:
        result["error"] = str(e)
        result["traceback"] = traceback.format_exc()

    test_results["load_and_prepare_train_dataset"] = result


def main():
    """Main entry point for data collection."""
    print("Collecting A4 Q1 Data...")
    print("=" * 60)

    print(f"Loading tokenizer: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    collect_encode_function_data(tokenizer)
    encode_status = "pass" if test_results["encode_function"]["error"] is None else "FAIL"
    print(f"encode_function: {encode_status}")
    for tc in test_results["encode_function"]["test_cases"]:
        tc_status = "pass" if tc.get("error") is None else "FAIL"
        print(f"  {tc['name']}: {tc_status}")
        if tc.get("error"):
            print(f"    Error: {tc['error']}")

    collect_load_and_prepare_data(tokenizer)
    ds_status = "pass" if test_results["load_and_prepare_train_dataset"]["error"] is None else "FAIL"
    print(f"load_and_prepare_train_dataset: {ds_status}")
    for tc in test_results["load_and_prepare_train_dataset"].get("test_cases", []):
        tc_status = "pass" if tc.get("error") is None else "FAIL"
        print(f"  {tc['name']}: {tc_status}")
        if tc.get("error"):
            print(f"    Error: {tc['error']}")

    # Save results
    output_path = Path(__file__).parent / "A4-Q1.json"
    with open(output_path, "w") as f:
        json.dump(test_results, f, indent=2)

    print("=" * 60)
    print(f"Results saved to: {output_path}")
    print("=" * 60)
    print("\nSubmit both data_utils.py and A4-Q1.json to Gradescope")


if __name__ == "__main__":
    main()
